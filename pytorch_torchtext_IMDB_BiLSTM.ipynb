{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_torchtext_IMDB_BiLSTM",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjOp4Qw3UtMHcIdl0Lv/n8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnXiaoNuan/geektime_learn_NLP/blob/master/pytorch_torchtext_IMDB_BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0koMWQ7IMmAj"
      },
      "source": [
        "# initialize environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHe6Ze7KXro"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchtext.legacy import data, datasets\r\n",
        "\r\n",
        "\r\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFEzmLbhFqBh"
      },
      "source": [
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDunT1RGRX1G"
      },
      "source": [
        "SEED = 1234\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPCOgeCbMXqn"
      },
      "source": [
        "# prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hFZpJj7O1N5"
      },
      "source": [
        "tokenize, build vocabulary, covert text into word index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg1c80b7TSsh"
      },
      "source": [
        "Field defines how to process text, here is the most common parameters:\r\n",
        "\r\n",
        "sequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\r\n",
        "\r\n",
        "use_vocab – Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.\r\n",
        "\r\n",
        "preprocessing – The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.\r\n",
        "\r\n",
        "batch_first – Whether to produce tensors with the batch dimension first. Default: False.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz5toNH7K2cL",
        "outputId": "7fed64f8-0630-4c04-e97a-71757938ecf7"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "tokenizer = word_tokenize"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tClMLTTDMW5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3021ab3-f0ee-451e-f750-a49faa15e25b"
      },
      "source": [
        "TEXT = data.Field(tokenize=tokenizer, include_lengths=True)\r\n",
        "LABEL = data.LabelField(dtype=torch.float)\r\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root='/home')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 23.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0eVh4_xMRk0"
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06wUuisFM_5T",
        "outputId": "e94db1e2-04b7-45c4-f255-5568e6131639"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Dataset.__getattr__ at 0x7f3e9d089dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raMDngUsLI5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4cd7f0-cd69-40cf-9372-37076e73e56d"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\r\n",
        "TEXT.build_vocab(train_data, max_size= MAX_VOCAB_SIZE, vectors=\"glove.6B.300d\", unk_init=torch.Tensor.normal_)\r\n",
        "LABEL.build_vocab(train_data, valid_data, test_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:46, 5.18MB/s]                           \n",
            "100%|█████████▉| 399848/400000 [00:35<00:00, 11279.15it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqFYN7KJSOxo"
      },
      "source": [
        "# build iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRXS7DIhOBMM",
        "outputId": "9ba6f373-091c-4212-9e98-4addbe135182"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)\r\n",
        "\r\n",
        "\r\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data),\r\n",
        "    batch_sizes = (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\r\n",
        "    sort_within_batch = True,\r\n",
        "    device = device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU8pfFwaUkR_"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUpHePwoTFI5"
      },
      "source": [
        "\r\n",
        "class BiLSTM(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout, pad_idx):\r\n",
        "        \r\n",
        "    super().__init__()\r\n",
        "    \r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\r\n",
        "    \r\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\r\n",
        "    \r\n",
        "    self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "    \r\n",
        "    self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "  def forward(self, text, text_lengths):\r\n",
        "      \r\n",
        "    embedded = self.embedding(text)\r\n",
        "    \r\n",
        "    #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\r\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\r\n",
        "    \r\n",
        "    packed_output, (hidden, cell) = self.rnn(packed_embedded)\r\n",
        "    \r\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output) \r\n",
        "    \r\n",
        "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # 双向LSTM\r\n",
        "            \r\n",
        "    return self.fc(hidden)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coj0AN7H_wHC"
      },
      "source": [
        "# initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J_1vzfMf59m"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\r\n",
        "EMBEDDING_DIM = 300\r\n",
        "HIDDEN_DIM = 256\r\n",
        "OUTPUT_DIM = 1\r\n",
        "N_LAYERS = 2\r\n",
        "BIDIRECTIONAL = True\r\n",
        "DROPOUT = 0.2\r\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "model = BiLSTM(INPUT_DIM, \r\n",
        "            EMBEDDING_DIM, \r\n",
        "            HIDDEN_DIM, \r\n",
        "            OUTPUT_DIM, \r\n",
        "            N_LAYERS, \r\n",
        "            BIDIRECTIONAL, \r\n",
        "            DROPOUT, \r\n",
        "            PAD_IDX)\r\n",
        "model = model.to(device)\r\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\r\n",
        "\r\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ZXG39QFYgp"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGhQRZUQ_2Z6"
      },
      "source": [
        "# initialize optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AytwCqMt__1W"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnvKY9nnAg7x"
      },
      "source": [
        "def binary_accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #round predictions to the closest integer\r\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\r\n",
        "    #print(rounded_preds)\r\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uONNAnnGAnSJ"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssJ-feOjAqUk"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, epoch, validate_after_n_batch):\r\n",
        "\r\n",
        "  best_valid_loss = float('inf')\r\n",
        "\r\n",
        "  n_batch_train_loss = 0\r\n",
        "  n_batch_train_acc = 0\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  total_batch = 0\r\n",
        "\r\n",
        "  start_time = time.time()\r\n",
        "\r\n",
        "  for batch in iterator:\r\n",
        "    \r\n",
        "    #print(batch)\r\n",
        "    total_batch += 1\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    text, text_lengths = batch.text\r\n",
        "\r\n",
        "    #print(text.shape)\r\n",
        "\r\n",
        "    text = text.to(device)\r\n",
        "\r\n",
        "    logits = model(text, text_lengths).squeeze(1)\r\n",
        "\r\n",
        "    #label = batch.label.type(torch.long)\r\n",
        "\r\n",
        "    loss = criterion(logits, batch.label)\r\n",
        "\r\n",
        "    acc = binary_accuracy(logits, batch.label)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    end_time = time.time()\r\n",
        "\r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "\r\n",
        "    n_batch_train_loss += loss.item()\r\n",
        "    n_batch_train_acc += acc.item()\r\n",
        "\r\n",
        "    avg_batch_train_loss = n_batch_train_loss / total_batch\r\n",
        "    avg_batch_train_acc = n_batch_train_acc / total_batch\r\n",
        "\r\n",
        "    if total_batch % validate_after_n_batch == 0:\r\n",
        "      # validation\r\n",
        "      avg_batch_valid_loss, avg_batch_valid_acc = evaluate(model, val_iter, criterion)\r\n",
        "\r\n",
        "      if avg_batch_valid_loss < best_valid_loss:\r\n",
        "          best_valid_loss = avg_batch_valid_loss\r\n",
        "          torch.save(model.state_dict(), 'model.pt')\r\n",
        "      print(f'Epoch: {epoch+1:02} | Total Batch: {total_batch:06} | Training Time for latest {validate_after_n_batch:03} batches: {epoch_mins}m {epoch_secs}s' )\r\n",
        "      print(f'\\tTrain Loss: {avg_batch_train_loss:.3f} | Train Acc: {avg_batch_train_acc*100:.2f}%')\r\n",
        "      print(f'\\t Val. Loss: {avg_batch_valid_loss:.3f} |  Val. Acc: {avg_batch_valid_acc*100:.2f}%')\r\n",
        "      start_time = time.time()\r\n",
        "      model.train()\r\n",
        "        \r\n",
        "  #return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxlfT6wKCUoR"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    total_loss = 0\r\n",
        "    total_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    #print(len(iterator))\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "          \r\n",
        "          text, text_lengths = batch.text\r\n",
        "\r\n",
        "          text = text.to(device)\r\n",
        "\r\n",
        "          #label = batch.label.type(torch.long)\r\n",
        "          \r\n",
        "          logits = model(text, text_lengths).squeeze(1)\r\n",
        "          \r\n",
        "          loss = criterion(logits, batch.label)\r\n",
        "          #print('loss:', loss)\r\n",
        "          \r\n",
        "          acc = binary_accuracy(logits, batch.label)\r\n",
        "          #print('acc:', acc)\r\n",
        "\r\n",
        "          total_loss += loss.item()\r\n",
        "          total_acc += acc.item()\r\n",
        "        \r\n",
        "    return total_loss / len(iterator), total_acc / len(iterator)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enr8DiUQiR1i"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjzg9ITVCZBR",
        "outputId": "b95da258-f6dc-450d-c564-62f565e7e036"
      },
      "source": [
        "\r\n",
        "N_EPOCHS = 5\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    validate_after_n_batch = 50\r\n",
        "    train(model, train_iter, optimizer, criterion, epoch, validate_after_n_batch)        "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Total Batch: 000050 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.676 | Train Acc: 56.47%\n",
            "\t Val. Loss: 0.640 |  Val. Acc: 63.91%\n",
            "Epoch: 01 | Total Batch: 000100 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.638 | Train Acc: 62.03%\n",
            "\t Val. Loss: 0.534 |  Val. Acc: 73.39%\n",
            "Epoch: 01 | Total Batch: 000150 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.626 | Train Acc: 64.34%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 69.81%\n",
            "Epoch: 01 | Total Batch: 000200 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.628 | Train Acc: 64.72%\n",
            "\t Val. Loss: 0.678 |  Val. Acc: 54.17%\n",
            "Epoch: 01 | Total Batch: 000250 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.633 | Train Acc: 63.88%\n",
            "\t Val. Loss: 0.630 |  Val. Acc: 64.15%\n",
            "Epoch: 02 | Total Batch: 000050 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.577 | Train Acc: 69.03%\n",
            "\t Val. Loss: 0.525 |  Val. Acc: 74.64%\n",
            "Epoch: 02 | Total Batch: 000100 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.492 | Train Acc: 75.30%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 85.23%\n",
            "Epoch: 02 | Total Batch: 000150 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.457 | Train Acc: 78.08%\n",
            "\t Val. Loss: 0.387 |  Val. Acc: 84.21%\n",
            "Epoch: 02 | Total Batch: 000200 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.431 | Train Acc: 80.00%\n",
            "\t Val. Loss: 0.341 |  Val. Acc: 86.00%\n",
            "Epoch: 02 | Total Batch: 000250 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.410 | Train Acc: 81.23%\n",
            "\t Val. Loss: 0.302 |  Val. Acc: 87.99%\n",
            "Epoch: 03 | Total Batch: 000050 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.236 | Train Acc: 90.72%\n",
            "\t Val. Loss: 0.312 |  Val. Acc: 87.33%\n",
            "Epoch: 03 | Total Batch: 000100 | Training Time for latest 050 batches: 0m 6s\n",
            "\tTrain Loss: 0.217 | Train Acc: 91.81%\n",
            "\t Val. Loss: 0.308 |  Val. Acc: 88.33%\n",
            "Epoch: 03 | Total Batch: 000150 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.217 | Train Acc: 91.74%\n",
            "\t Val. Loss: 0.287 |  Val. Acc: 88.49%\n",
            "Epoch: 03 | Total Batch: 000200 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.222 | Train Acc: 91.55%\n",
            "\t Val. Loss: 0.337 |  Val. Acc: 85.50%\n",
            "Epoch: 03 | Total Batch: 000250 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.220 | Train Acc: 91.56%\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 89.08%\n",
            "Epoch: 04 | Total Batch: 000050 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.148 | Train Acc: 94.88%\n",
            "\t Val. Loss: 0.327 |  Val. Acc: 87.32%\n",
            "Epoch: 04 | Total Batch: 000100 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.146 | Train Acc: 94.86%\n",
            "\t Val. Loss: 0.289 |  Val. Acc: 88.67%\n",
            "Epoch: 04 | Total Batch: 000150 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.141 | Train Acc: 95.01%\n",
            "\t Val. Loss: 0.287 |  Val. Acc: 89.01%\n",
            "Epoch: 04 | Total Batch: 000200 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.144 | Train Acc: 94.87%\n",
            "\t Val. Loss: 0.311 |  Val. Acc: 88.72%\n",
            "Epoch: 04 | Total Batch: 000250 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.152 | Train Acc: 94.47%\n",
            "\t Val. Loss: 0.459 |  Val. Acc: 79.88%\n",
            "Epoch: 05 | Total Batch: 000050 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.111 | Train Acc: 95.75%\n",
            "\t Val. Loss: 0.383 |  Val. Acc: 87.01%\n",
            "Epoch: 05 | Total Batch: 000100 | Training Time for latest 050 batches: 0m 6s\n",
            "\tTrain Loss: 0.104 | Train Acc: 96.33%\n",
            "\t Val. Loss: 0.364 |  Val. Acc: 88.51%\n",
            "Epoch: 05 | Total Batch: 000150 | Training Time for latest 050 batches: 0m 7s\n",
            "\tTrain Loss: 0.099 | Train Acc: 96.69%\n",
            "\t Val. Loss: 0.372 |  Val. Acc: 87.27%\n",
            "Epoch: 05 | Total Batch: 000200 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.098 | Train Acc: 96.80%\n",
            "\t Val. Loss: 0.363 |  Val. Acc: 88.63%\n",
            "Epoch: 05 | Total Batch: 000250 | Training Time for latest 050 batches: 0m 8s\n",
            "\tTrain Loss: 0.107 | Train Acc: 96.43%\n",
            "\t Val. Loss: 0.450 |  Val. Acc: 83.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_vdhLYN_Kvf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKUlyUmiCuvq"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\r\n",
        "\r\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}