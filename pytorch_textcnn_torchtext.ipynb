{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_textcnn_torchtext",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbShLWHHU947H9QUeSr1t7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnXiaoNuan/geektime_learn_NLP/blob/master/pytorch_textcnn_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHe6Ze7KXro"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchtext.legacy import data, datasets\r\n",
        "\r\n",
        "\r\n",
        "import random"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFEzmLbhFqBh"
      },
      "source": [
        "import time"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDunT1RGRX1G"
      },
      "source": [
        "SEED = 1234\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPCOgeCbMXqn"
      },
      "source": [
        "# prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hFZpJj7O1N5"
      },
      "source": [
        "tokenize, build vocabulary, covert text into word index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg1c80b7TSsh"
      },
      "source": [
        "Field defines how to process text, here is the most common parameters:\r\n",
        "\r\n",
        "sequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\r\n",
        "\r\n",
        "use_vocab – Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.\r\n",
        "\r\n",
        "preprocessing – The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.\r\n",
        "\r\n",
        "batch_first – Whether to produce tensors with the batch dimension first. Default: False.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz5toNH7K2cL",
        "outputId": "4cfde8db-a16e-4ac7-fcb9-32882cd658ff"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "tokenizer = word_tokenize"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tClMLTTDMW5a"
      },
      "source": [
        "TEXT = data.Field(tokenize=tokenizer, include_lengths=True)\r\n",
        "LABEL = data.LabelField(dtype=torch.float)\r\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root='/home')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0eVh4_xMRk0"
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raMDngUsLI5v"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\r\n",
        "TEXT.build_vocab(train_data, max_size= MAX_VOCAB_SIZE, vectors=\"glove.6B.300d\", unk_init=torch.Tensor.normal_)\r\n",
        "LABEL.build_vocab(train_data, valid_data, test_data)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqFYN7KJSOxo"
      },
      "source": [
        "# build iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRXS7DIhOBMM",
        "outputId": "cb2f402e-0da2-4a9a-fe4c-456a7f2af0e5"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)\r\n",
        "\r\n",
        "\r\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data),\r\n",
        "    batch_sizes = (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\r\n",
        "    sort_within_batch = True,\r\n",
        "    device = device)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU8pfFwaUkR_"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUpHePwoTFI5"
      },
      "source": [
        "\r\n",
        "class textCNNMulti(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, args):\r\n",
        "    super().__init__()\r\n",
        "    dim = args['dim']\r\n",
        "    n_class = args['n_class']\r\n",
        "    embeddings=args['embedding_matrix']\r\n",
        "    kernels = [3, 4, 5]\r\n",
        "    kernel_number = [150, 150, 150]\r\n",
        "    self.static_embed = nn.Embedding.from_pretrained(embeddings)\r\n",
        "    self.non_static_embed = nn.Embedding.from_pretrained(embeddings, freeze=False)\r\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(2, number, (size, dim), padding=(size-1,0)) for (size, number) in zip(kernels, kernel_number)])\r\n",
        "    self.dropout = nn.Dropout()\r\n",
        "    self.out = nn.Linear(sum(kernel_number), n_class)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    #print('x original shape is ', x.shape)\r\n",
        "    non_static_input = self.non_static_embed(x)\r\n",
        "    static_input = self.static_embed(x)\r\n",
        "    x = torch.stack([non_static_input, static_input], dim=1)\r\n",
        "    x = x.permute(2, 1, 0, 3)\r\n",
        "    #print('x after being stacked shape is ',x.shape)\r\n",
        "    conv_pool_x = []\r\n",
        "    for conv in self.convs:\r\n",
        "      relu_x = nn.functional.relu(conv(x))\r\n",
        "      #print('relu_x before squeezing', relu_x.shape)\r\n",
        "      relu_x = relu_x.squeeze(3)\r\n",
        "      #print('relu_x after squeezing', relu_x.shape)\r\n",
        "      pool_x = nn.functional.max_pool1d(relu_x, relu_x.size(2))\r\n",
        "      #print('pool_x before squeezing', relu_x.shape)\r\n",
        "      pool_x = pool_x.squeeze(2)\r\n",
        "      #print('pool_x after squeezing', relu_x.shape)\r\n",
        "      conv_pool_x.append(pool_x)\r\n",
        "    \r\n",
        "\r\n",
        "    #print('len(conv_pool_x):', len(conv_pool_x))\r\n",
        "    #print('conv_pool_x[0].shape:', conv_pool_x[0].shape)\r\n",
        "    conv_pool_x = torch.cat(conv_pool_x, 1)\r\n",
        "    #print('conv_pool_x:', conv_pool_x.shape)\r\n",
        "    conv_pool_x = self.dropout(conv_pool_x)\r\n",
        "    #print('conv_pool_x after dropout:', conv_pool_x.shape)\r\n",
        "    conv_pool_x = self.out(conv_pool_x)\r\n",
        "    return conv_pool_x\r\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQl897eJcttO",
        "outputId": "035f4366-c5bd-4c3f-8eea-f6b4906ca67e"
      },
      "source": [
        "a = torch.tensor([[1,2,3], [4, 5, 6]])\r\n",
        "b = torch.tensor([[7, 8, 9], [11, 12, 13]])\r\n",
        "c = torch.stack([a, b], dim=1)\r\n",
        "c"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1,  2,  3],\n",
              "         [ 7,  8,  9]],\n",
              "\n",
              "        [[ 4,  5,  6],\n",
              "         [11, 12, 13]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM-GO_bIdDH8",
        "outputId": "bead0f71-0db3-40d3-f56b-bde2b6ae2412"
      },
      "source": [
        "c.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coj0AN7H_wHC"
      },
      "source": [
        "# initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J_1vzfMf59m"
      },
      "source": [
        "args={}\r\n",
        "args['vocb_size']=len(TEXT.vocab)\r\n",
        "args['dim']=300\r\n",
        "args['n_class']=len(LABEL.vocab)\r\n",
        "args['embedding_matrix']=TEXT.vocab.vectors\r\n",
        "args['lr']=0.001\r\n",
        "args['momentum']=0.8\r\n",
        "args['epochs']=180\r\n",
        "args['log_interval']=100\r\n",
        "args['test_interval']=500\r\n",
        "args['save_dir']='./'\r\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "model = textCNNMulti(args)\r\n",
        "model = model.to(device)\r\n",
        "model.static_embed.weight.data[UNK_IDX] = torch.zeros(args['dim'])\r\n",
        "model.static_embed.weight.data[PAD_IDX] = torch.zeros(args['dim'])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ZXG39QFYgp"
      },
      "source": [
        "model.non_static_embed.weight.data[UNK_IDX] = torch.zeros(args['dim'])\r\n",
        "model.non_static_embed.weight.data[PAD_IDX] = torch.zeros(args['dim'])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGhQRZUQ_2Z6"
      },
      "source": [
        "# initialize optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AytwCqMt__1W"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnvKY9nnAg7x"
      },
      "source": [
        "def binary_accuracy(logits, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #round predictions to the closest integer\r\n",
        "    softmax = nn.Softmax(dim=1)\r\n",
        "    probs = softmax(logits)\r\n",
        "    _, y_pred_tags = torch.max(probs, dim = 1)\r\n",
        "    correct = (y_pred_tags == y).float() #convert into float for division \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uONNAnnGAnSJ"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssJ-feOjAqUk"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, epoch, validate_after_n_batch):\r\n",
        "  # epoch_loss = 0\r\n",
        "  # epoch_acc = 0\r\n",
        "\r\n",
        "  best_valid_loss = float('inf')\r\n",
        "\r\n",
        "  n_batch_train_loss = 0\r\n",
        "  n_batch_train_acc = 0\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  total_batch = 0\r\n",
        "\r\n",
        "  start_time = time.time()\r\n",
        "\r\n",
        "  for batch in iterator:\r\n",
        "    \r\n",
        "    #print(batch)\r\n",
        "    total_batch += 1\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    text, tex_lengths = batch.text\r\n",
        "\r\n",
        "    #print(text.shape)\r\n",
        "\r\n",
        "    text = text.cuda()\r\n",
        "\r\n",
        "    logits = model(text).squeeze(1)\r\n",
        "\r\n",
        "    label = batch.label.type(torch.long)\r\n",
        "\r\n",
        "    loss = criterion(logits, label)\r\n",
        "\r\n",
        "    acc = binary_accuracy(logits, label)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    end_time = time.time()\r\n",
        "\r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "\r\n",
        "    n_batch_train_loss += loss.item()\r\n",
        "    n_batch_train_acc += acc.item()\r\n",
        "\r\n",
        "    avg_batch_train_loss = n_batch_train_loss / total_batch\r\n",
        "    avg_batch_train_acc = n_batch_train_acc / total_batch\r\n",
        "\r\n",
        "    # validation\r\n",
        "    avg_batch_valid_loss, avg_batch_valid_acc = evaluate(model, val_iter, criterion)\r\n",
        "\r\n",
        "    if avg_batch_valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = avg_batch_valid_loss\r\n",
        "        torch.save(model.state_dict(), 'model.pt')\r\n",
        "\r\n",
        "    if total_batch % validate_after_n_batch == 0:\r\n",
        "      print(f'Epoch: {epoch+1:02} | Total Batch: {total_batch:06} | Training Time for latest {validate_after_n_batch:03} batches: {epoch_mins}m {epoch_secs}s' )\r\n",
        "      print(f'\\tTrain Loss: {avg_batch_train_loss:.3f} | Train Acc: {avg_batch_train_acc*100:.2f}%')\r\n",
        "      print(f'\\t Val. Loss: {avg_batch_valid_loss:.3f} |  Val. Acc: {avg_batch_valid_acc*100:.2f}%')\r\n",
        "      start_time = time.time()\r\n",
        "        \r\n",
        "  #return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxlfT6wKCUoR"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    total_loss = 0\r\n",
        "    total_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    #print(len(iterator))\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "          \r\n",
        "          text, text_lengths = batch.text\r\n",
        "\r\n",
        "          text = text.cuda()\r\n",
        "\r\n",
        "          label = batch.label.type(torch.long)\r\n",
        "          \r\n",
        "          logits = model(text).squeeze(1)\r\n",
        "          \r\n",
        "          loss = criterion(logits, label)\r\n",
        "          #print('loss:', loss)\r\n",
        "          \r\n",
        "          acc = binary_accuracy(logits, batch.label)\r\n",
        "          #print('acc:', acc)\r\n",
        "\r\n",
        "          total_loss += loss.item()\r\n",
        "          total_acc += acc.item()\r\n",
        "        \r\n",
        "    return total_loss / len(iterator), total_acc / len(iterator)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enr8DiUQiR1i"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjzg9ITVCZBR",
        "outputId": "bcf66098-2530-496b-e4ef-d7396c3c58a6"
      },
      "source": [
        "\r\n",
        "N_EPOCHS = 5\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    validate_after_n_batch = 50\r\n",
        "    train(model, train_iter, optimizer, criterion, epoch, validate_after_n_batch)        "
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Total Batch: 000050 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.404 |  Val. Acc: 87.62%\n",
            "Epoch: 01 | Total Batch: 000100 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.89%\n",
            "\t Val. Loss: 0.419 |  Val. Acc: 87.50%\n",
            "Epoch: 01 | Total Batch: 000150 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.93%\n",
            "\t Val. Loss: 0.398 |  Val. Acc: 87.91%\n",
            "Epoch: 01 | Total Batch: 000200 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.94%\n",
            "\t Val. Loss: 0.399 |  Val. Acc: 88.11%\n",
            "Epoch: 01 | Total Batch: 000250 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.94%\n",
            "\t Val. Loss: 0.402 |  Val. Acc: 88.18%\n",
            "Epoch: 02 | Total Batch: 000050 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.412 |  Val. Acc: 87.92%\n",
            "Epoch: 02 | Total Batch: 000100 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.89%\n",
            "\t Val. Loss: 0.449 |  Val. Acc: 87.52%\n",
            "Epoch: 02 | Total Batch: 000150 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.93%\n",
            "\t Val. Loss: 0.409 |  Val. Acc: 88.20%\n",
            "Epoch: 02 | Total Batch: 000200 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.412 |  Val. Acc: 88.31%\n",
            "Epoch: 02 | Total Batch: 000250 | Training Time for latest 050 batches: 2m 42s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.96%\n",
            "\t Val. Loss: 0.414 |  Val. Acc: 88.36%\n",
            "Epoch: 03 | Total Batch: 000050 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.78%\n",
            "\t Val. Loss: 0.452 |  Val. Acc: 87.96%\n",
            "Epoch: 03 | Total Batch: 000100 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.88%\n",
            "\t Val. Loss: 0.441 |  Val. Acc: 88.07%\n",
            "Epoch: 03 | Total Batch: 000150 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.005 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.438 |  Val. Acc: 88.14%\n",
            "Epoch: 03 | Total Batch: 000200 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.94%\n",
            "\t Val. Loss: 0.439 |  Val. Acc: 88.23%\n",
            "Epoch: 03 | Total Batch: 000250 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.441 |  Val. Acc: 88.08%\n",
            "Epoch: 04 | Total Batch: 000050 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.88%\n",
            "\t Val. Loss: 0.466 |  Val. Acc: 88.10%\n",
            "Epoch: 04 | Total Batch: 000100 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.004 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.491 |  Val. Acc: 87.79%\n",
            "Epoch: 04 | Total Batch: 000150 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.474 |  Val. Acc: 88.02%\n",
            "Epoch: 04 | Total Batch: 000200 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.003 | Train Acc: 99.96%\n",
            "\t Val. Loss: 0.465 |  Val. Acc: 88.23%\n",
            "Epoch: 04 | Total Batch: 000250 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.002 | Train Acc: 99.97%\n",
            "\t Val. Loss: 0.461 |  Val. Acc: 88.39%\n",
            "Epoch: 05 | Total Batch: 000050 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.66%\n",
            "\t Val. Loss: 0.570 |  Val. Acc: 87.02%\n",
            "Epoch: 05 | Total Batch: 000100 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.70%\n",
            "\t Val. Loss: 0.520 |  Val. Acc: 87.80%\n",
            "Epoch: 05 | Total Batch: 000150 | Training Time for latest 050 batches: 2m 44s\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.54%\n",
            "\t Val. Loss: 0.581 |  Val. Acc: 86.19%\n",
            "Epoch: 05 | Total Batch: 000200 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.054 | Train Acc: 98.46%\n",
            "\t Val. Loss: 1.099 |  Val. Acc: 81.43%\n",
            "Epoch: 05 | Total Batch: 000250 | Training Time for latest 050 batches: 2m 43s\n",
            "\tTrain Loss: 0.061 | Train Acc: 98.18%\n",
            "\t Val. Loss: 0.709 |  Val. Acc: 85.96%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_vdhLYN_Kvf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKUlyUmiCuvq"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\r\n",
        "\r\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}